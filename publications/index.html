<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Michael  Beyer | publications</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://m-beyer.github.io/">
       <span class="font-weight-bold">Michael</span>   Beyer
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ESREL PSAM</abbr>
    
  
  </div>

  <div id="Beyer2020" class="col-sm-8">
    
      <div class="title">Fault Injectors for TensorFlow: Evaluation of the Impact of Random Hardware Faults on Deep CNNs</div>
      <div class="author">
        
          
            
              
                <em>Beyer, M.</em>,
              
            
          
        
          
            
              
                
                  Morozov, A.,
                
              
            
          
        
          
            
              
                
                  Valiev, E.,
                
              
            
          
        
          
            
              
                
                  Schorn, C.,
                
              
            
          
        
          
            
              
                
                  Gauerhof, L.,
                
              
            
          
        
          
            
              
                
                  Ding, K.,
                
              
            
          
        
          
            
              
                
                  and Janschek, K.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 30th European Safety and Reliability Conference and 15th Probabilistic Safety Assessment and Management Conference (ESREL2020 PSAM15)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2012.07037" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="https://www.rpsonline.com.sg/proceedings/esrel2020/pdf/5754.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/mbsa-tud/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Today, Deep Learning (DL) enhances almost every industrial sector, including safety-critical areas. The next generation of safety standards will define appropriate verification techniques for DL-based applications and propose adequate fault tolerance mechanisms. DL-based applications, like any other software, are susceptible to common random hardware faults such as bit flips, which occur in RAM and CPU registers. Such faults can lead to silent data corruption. Therefore, it is crucial to develop methods and tools that help to evaluate how DL components operate under the presence of such faults. In this paper, we introduce two new Fault Injection (FI) frameworks InjectTF and InjectTF2 for TensorFlow 1 and TensorFlow 2, respectively. Both frameworks are available on GitHub and allow the configurable injection of random faults into Neural Networks (NN). In order to demonstrate the feasibility of the frameworks, we also present the results of FI experiments conducted on four VGG-based Convolutional NNs using two image sets. The results demonstrate how random bit flips in the output of particular mathematical operations and layers of NNs affect the classification accuracy. These results help to identify the most critical operations and layers, compare the reliability characteristics of functionally similar NNs, and introduce selective fault tolerance mechanisms.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Morozov2020" class="col-sm-8">
    
      <div class="title">Bayesian Model for Trustworthiness Analysis of Deep Learning Classifiers</div>
      <div class="author">
        
          
            
              
                
                  Morozov, A.,
                
              
            
          
        
          
            
              
                
                  Valiev, E.,
                
              
            
          
        
          
            
              
                <em>Beyer, M.</em>,
              
            
          
        
          
            
              
                
                  Ding, K.,
                
              
            
          
        
          
            
              
                
                  Gauerhof, L.,
                
              
            
          
        
          
            
              
                
                  and Schorn, C.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Workshop on Artificial Intelligence Safety 2020 co-located with the 29th International Joint Conference on Artificial Intelligence and the 17th Pacific Rim International Conference on Artificial Intelligence (IJCAI-PRICAI 2020)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="http://ceur-ws.org/Vol-2640/paper_6.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/mbsa-tud/InjectTF2" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the near future, Artificial Intelligence methods will inevitably enter safety-critical areas. Deep Learning software, deployed on standard comput- ing hardware, is prone to random hardware faults such as bit flips that can result in silent data cor- ruption. We have performed fault injection ex- periments on three Convolution Neural Network (CNN) image classifiers, including VGG16 and VGG19. Besides the fact that the bit flips indeed drop the classification accuracy, we have observed that these faults result not in random misclassifica- tion but tend to particular erroneous sets of classes. This fact shall be taken into account to design a reliable and safe system. For example, we might consider re-running the classifier if it yields a class for such an erroneous set. This paper discusses the results of our fault injection experiments and introduces a new Bayesian Network (BN) model that aggregates these results and enables numeri- cal evaluation of the performance of the CNNs un- der the influence of random hardware faults. We demonstrate the application of the developed BN model for the trustworthiness analysis. In particu- lar, we show how to evaluate the misclassification probabilities for each resulting class, for the vary- ing probability of random bit-flips.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="valiev2020evaluation" class="col-sm-8">
    
      <div class="title">Evaluation of the Impact of Random Computing Hardware Faults on the Performance of Convolutional Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Valiev, E.,
                
              
            
          
        
          
            
              
                
                  Morozov, A.,
                
              
            
          
        
          
            
              
                <em>Beyer, M.</em>,
              
            
          
        
          
            
              
                
                  Yusupova, N.,
                
              
            
          
        
          
            
              
                
                  and Janschek, K.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 8th Scientific Conference on Information Technologies for Intelligent Decision Making Support (ITIDS 2020)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://download.atlantis-press.com/proceedings/itids-20/125946024" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/mbsa-tud/InjectTF2" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Artificial Intelligence (AI) rapidly spreads across high-tech industries and enters almost every safety-critical area such as automotive, aerospace, and medical industries. However, like any other software, AI-based applications are prone to random hardware faults such as a random bit flip in CPU, RAM, or network. Therefore, it is essential to understand how various hardware faults affect the performance and accuracy of AI applications. This paper provides a general description and particular conceptual and implementational features of our recently introduced Fault Injection (FI) framework InjectTF2. InjectTF2 is developed using the TensorFlow 2 API and allows the user to specify fault parameters and perform layer-wise fault injection into the TensorFlow 2 neural networks. It enables the automated injection of random bitflips. The paper describes the software architecture of the framework. The framework is open source and freely available on the GitHub. The application of InjectTF2 is demonstrated with extensive fault injection experiments on a Convolutional Neural Network (CNN) trained using the GTSRB dataset. The experiments’ results show how random bitflips in the outputs of the CNNs layers affect the classification accuracy. Such results support not only numerical analysis of reliability and safety characteristics but also help to identify the most critical CNN layers for more robust and fault-tolerant design.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISSRE</abbr>
    
  
  </div>

  <div id="8990333" class="col-sm-8">
    
      <div class="title">Quantification of the Impact of Random Hardware Faults on Safety-Critical AI Applications: CNN-Based Traffic Sign Recognition Case Study</div>
      <div class="author">
        
          
            
              
                <em>Beyer, M.</em>,
              
            
          
        
          
            
              
                
                  Morozov, A.,
                
              
            
          
        
          
            
              
                
                  Ding, K.,
                
              
            
          
        
          
            
              
                
                  Ding, S.,
                
              
            
          
        
          
            
              
                
                  and Janschek, K.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/8990333/" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/mbsa-tud/InjectTF" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Nowadays, Artificial Intelligence (AI) rapidly enters almost every safety-critical domain, including the automotive industry. The next generation of functional safety standards has to define appropriate verification and validation techniques and propose adequate fault tolerance mechanisms. Several AI frameworks, such as TensorFlow by Google, have already proven to be effective and reliable platforms. However, similar to any other software, AI-based applications are prone to common random hardware faults, e.g., bit-flips which may occur in RAM or CPU registers and might lead to silent data corruption. Therefore, it is crucial to understand how different hardware faults affect the accuracy of AI applications. This paper introduces our new fault injection framework for TensorFlow and results of first experiments conducted on a Convolutional Neural Network (CNN) based traffic sign classifier. These results demonstrate the feasibility of the fault injection framework. In particular, they help to identify the most critical parts of a neural network under test.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Michael  Beyer.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
